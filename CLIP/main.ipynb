{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import clip\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_chunk_size = 1000\n",
    "img_chunk_size = 1024\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names = pd.read_csv('{csv path}/classes_in_imagenet.csv')['class_name'] # https://github.com/mf1024/ImageNet-datasets-downloader/blob/master/classes_in_imagenet.csv\n",
    "text_descriptions = [f\"This is a photo of a {label}\" for label in class_names]\n",
    "text_features = []\n",
    "\n",
    "# use chunk\n",
    "with torch.no_grad():\n",
    "    for text_chunk in tqdm([text_descriptions[i:i + text_chunk_size] for i in range(0, len(text_descriptions), text_chunk_size)]):\n",
    "      text_tokens = clip.tokenize(text_chunk).cuda()\n",
    "      tmp = model.encode_text(text_tokens).float()\n",
    "      tmp /= tmp.norm(dim=-1, keepdim=True)\n",
    "      text_features.append(tmp)\n",
    "text_features = torch.cat(tuple(text_features),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = []\n",
    "images = []\n",
    "\n",
    "img_paths = glob.glob('{images path}')\n",
    "preds = []\n",
    "\n",
    "for i, img_path in tqdm(enumerate(img_paths), total=len(img_paths)):\n",
    "    # train 이미지 불러오기\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    images.append(preprocess(image))\n",
    "    if (i+1) % img_chunk_size == 0:\n",
    "        image_input = torch.tensor(np.stack(images)).cuda()\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_input).float()\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            top_probs, top_labels = text_probs.cpu().topk(1, dim=-1)\n",
    "\n",
    "            preds += top_labels.flatten().tolist()\n",
    "        images = []"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
